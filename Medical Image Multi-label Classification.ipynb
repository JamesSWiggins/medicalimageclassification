{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for multi-label image classification\n",
    "\n",
    "Each original chest x-ray image is 1024×1024 in size. We used 224×224 input resolution to obtain initial results, and then we obtained results on the full resolution. The input dataset is split into training/validation and testing without patient overlap between these sets and the 14 diseases are modeled as 14 binary outputs of a multi-label classifier. Each disease output indicates the presence/absence of the disease and “no disease” is represented as zeros for all labels. We first split the data into three sets: training, validation, and testing, based on patient IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket='analytics-serverless-west'\n",
    "prefix = 'sagemaker/x-ray'\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "        \n",
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "def download_from_s3(bucket, key, local_file_name):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket).download_file(key, local_file_name)\n",
    "\n",
    "trainper = 0.7\n",
    "valper = 0.1\n",
    "file_name = 'Data_Entry_2017.csv'\n",
    "\n",
    "download_from_s3(bucket, prefix+'/'+file_name, file_name)\n",
    "\n",
    "a = pd.read_csv(file_name)\n",
    "patient_ids = a['Patient ID']\n",
    "uniq_pids = np.unique(patient_ids)\n",
    "np.random.shuffle(uniq_pids)\n",
    "total_ids = len(uniq_pids)\n",
    "#total_ids = 30000 #pick a sample size you want to use to train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
       "0                        0.143  0.143          NaN  \n",
       "1                        0.143  0.143          NaN  \n",
       "2                        0.168  0.168          NaN  \n",
       "3                        0.171  0.171          NaN  \n",
       "4                        0.143  0.143          NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train, Validation, and Testing datasets from the full image list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patient ids: training: 21563, validation: 3079, testing: 6161\n"
     ]
    }
   ],
   "source": [
    "trainset = int(trainper*total_ids)\n",
    "valset = trainset+int(valper*total_ids)\n",
    "testset = trainset+valset\n",
    "\n",
    "train = uniq_pids[:trainset]\n",
    "val = uniq_pids[trainset+1:valset]\n",
    "test = uniq_pids[valset+1:]\n",
    "print('Number of patient ids: training: %d, validation: %d, testing: %d'%(len(train), len(val), len(test)))\n",
    "\n",
    "traindata = a.loc[a['Patient ID'].isin(train)]\n",
    "valdata = a.loc[a['Patient ID'].isin(val)]\n",
    "testdata = a.loc[a['Patient ID'].isin(test)]\n",
    "\n",
    "traindata.to_csv('traindata.csv', sep=',', header=False, index=False)\n",
    "valdata.to_csv('valdata.csv', sep=',', header=False, index=False)\n",
    "testdata.to_csv('testdata.csv', sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the ‘[lst](https://mxnet.incubator.apache.org/faq/recordio.html)’ files for multi-label classification where each disease is mapped to a set of binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def gen_set(csvfile, outputfile):\n",
    "    disease_list = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', \\\n",
    "                   'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', \\\n",
    "                   'Hernia']\n",
    "    alldiseases = {disease:i for i,disease in enumerate(disease_list)}\n",
    "    with open(outputfile, 'w') as fp:\n",
    "        with open(csvfile, 'r') as cfile:\n",
    "            line = csv.reader(cfile, delimiter=',')\n",
    "            index = 0\n",
    "            for element in line:\n",
    "                # the first column is the image filename, while the second\n",
    "                # column has the list of diseases separated by |\n",
    "                diseases = element[1].split('|')\n",
    "                fp.write('%d\\t'%index)\n",
    "                for d in alldiseases:\n",
    "                    if d in diseases:\n",
    "                        fp.write('%d\\t'%1)\n",
    "                    else:\n",
    "                        fp.write('%d\\t'%0)\n",
    "                fp.write('images/%s\\n' % element[0])\n",
    "                index += 1\n",
    "                 \n",
    "gen_set('traindata.csv', 'chestxraytrain.lst')\n",
    "gen_set('valdata.csv', 'chestxrayval.lst')\n",
    "gen_set('testdata.csv', 'chestxraytest.lst')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet of the annotated training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00000001_000.png</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>58</th>\n",
       "      <th>M</th>\n",
       "      <th>PA</th>\n",
       "      <th>2682</th>\n",
       "      <th>2749</th>\n",
       "      <th>0.14300000000000002</th>\n",
       "      <th>0.14300000000000002.1</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_001.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   00000001_000.png            Cardiomegaly  0  1  58  M  PA  2682  2749  \\\n",
       "0  00000001_001.png  Cardiomegaly|Emphysema  1  1  58  M  PA  2894  2729   \n",
       "1  00000001_002.png   Cardiomegaly|Effusion  2  1  58  M  PA  2500  2048   \n",
       "2  00000002_000.png              No Finding  0  2  81  M  PA  2500  2048   \n",
       "3  00000003_000.png                  Hernia  0  3  81  F  PA  2582  2991   \n",
       "4  00000003_001.png                  Hernia  1  3  74  F  PA  2500  2048   \n",
       "\n",
       "   0.14300000000000002  0.14300000000000002.1  Unnamed: 11  \n",
       "0                0.143                  0.143          NaN  \n",
       "1                0.168                  0.168          NaN  \n",
       "2                0.171                  0.171          NaN  \n",
       "3                0.143                  0.143          NaN  \n",
       "4                0.168                  0.168          NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_csv = pd.read_csv('traindata.csv')\n",
    "v_csv = pd.read_csv('valdata.csv')\n",
    "\n",
    "t_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet of the annotated validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00000008_000.png</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>0</th>\n",
       "      <th>8</th>\n",
       "      <th>69</th>\n",
       "      <th>F</th>\n",
       "      <th>PA</th>\n",
       "      <th>2048</th>\n",
       "      <th>2500</th>\n",
       "      <th>0.171</th>\n",
       "      <th>0.171.1</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000008_001.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2048</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000008_002.png</td>\n",
       "      <td>Nodule</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2048</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000014_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>61</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2048</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000040_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>67</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000040_001.png</td>\n",
       "      <td>Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>67</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   00000008_000.png Cardiomegaly  0   8  69  F  PA  2048  2500  0.171  \\\n",
       "0  00000008_001.png   No Finding  1   8  70  F  PA  2048  2500  0.171   \n",
       "1  00000008_002.png       Nodule  2   8  73  F  PA  2048  2500  0.168   \n",
       "2  00000014_000.png   No Finding  0  14  61  F  PA  2048  2500  0.171   \n",
       "3  00000040_000.png   No Finding  0  40  67  M  PA  2500  2048  0.168   \n",
       "4  00000040_001.png    Emphysema  1  40  67  M  AP  2500  2048  0.168   \n",
       "\n",
       "   0.171.1  Unnamed: 11  \n",
       "0    0.171          NaN  \n",
       "1    0.168          NaN  \n",
       "2    0.171          NaN  \n",
       "3    0.168          NaN  \n",
       "4    0.168          NaN  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create recordio files using [im2rec.py](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py) for training and validation. We pass the option pack_label so that the recordio file is created as a multi-label file. For more details, please refer to the Amazon SageMaker multi-label image classification notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only run the below cell once. This is an expensive operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://analytics-serverless-west/sagemaker/x-ray/images/ images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000002_000.png\n",
      "1\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000009_000.png\n",
      "2\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000010_000.png\n",
      "3\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\timages/00000011_000.png\n",
      "4\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000011_001.png\n",
      "5\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000011_002.png\n",
      "6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000011_003.png\n",
      "7\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000011_004.png\n",
      "8\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000011_005.png\n",
      "9\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000011_006.png\n"
     ]
    }
   ],
   "source": [
    "!head -10 chestxraytest.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .rec file from /home/ec2-user/SageMaker/chestxraytrain.lst in /home/ec2-user/SageMaker\n",
      "multiprocessing not available, fall back to single threaded encoding\n",
      "time: 0.0586705207824707  count: 0\n",
      "libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
      "time: 53.599899768829346  count: 1000\n",
      "time: 53.78526711463928  count: 2000\n",
      "time: 54.013200521469116  count: 3000\n",
      "time: 53.6976683139801  count: 4000\n",
      "time: 53.34519624710083  count: 5000\n",
      "time: 54.07750463485718  count: 6000\n",
      "time: 53.62135601043701  count: 7000\n",
      "libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
      "time: 53.41580605506897  count: 8000\n",
      "time: 54.20130252838135  count: 9000\n",
      "time: 53.700379610061646  count: 10000\n",
      "time: 54.05881881713867  count: 11000\n",
      "time: 54.3556752204895  count: 12000\n",
      "time: 54.17163586616516  count: 13000\n",
      "time: 53.96117687225342  count: 14000\n",
      "time: 54.45259714126587  count: 15000\n",
      "time: 53.820563554763794  count: 16000\n",
      "time: 54.11435651779175  count: 17000\n",
      "time: 54.07949161529541  count: 18000\n",
      "time: 53.958163022994995  count: 19000\n",
      "time: 53.94041633605957  count: 20000\n",
      "time: 53.750375270843506  count: 21000\n",
      "time: 53.92852067947388  count: 22000\n",
      "time: 53.995617628097534  count: 23000\n",
      "time: 53.8095269203186  count: 24000\n",
      "time: 54.04582762718201  count: 25000\n",
      "time: 53.9742271900177  count: 26000\n",
      "time: 53.84714889526367  count: 27000\n",
      "time: 54.224936723709106  count: 28000\n",
      "time: 55.336997747421265  count: 29000\n",
      "time: 54.91517758369446  count: 30000\n",
      "time: 55.48112964630127  count: 31000\n",
      "time: 54.11932897567749  count: 32000\n",
      "time: 55.03017067909241  count: 33000\n",
      "time: 54.578964948654175  count: 34000\n",
      "time: 55.00973033905029  count: 35000\n",
      "time: 55.250133752822876  count: 36000\n",
      "time: 55.27975583076477  count: 37000\n",
      "time: 55.01710104942322  count: 38000\n",
      "time: 55.247395515441895  count: 39000\n",
      "time: 54.91483283042908  count: 40000\n",
      "time: 54.923710346221924  count: 41000\n",
      "time: 55.18356156349182  count: 42000\n",
      "time: 54.56795692443848  count: 43000\n",
      "time: 54.61714816093445  count: 44000\n",
      "time: 54.85934329032898  count: 45000\n",
      "time: 55.257863998413086  count: 46000\n",
      "time: 55.051440715789795  count: 47000\n",
      "time: 55.238242864608765  count: 48000\n",
      "time: 55.48592805862427  count: 49000\n",
      "time: 54.75011491775513  count: 50000\n",
      "time: 55.33035135269165  count: 51000\n",
      "time: 55.2620644569397  count: 52000\n",
      "time: 54.99716091156006  count: 53000\n",
      "time: 54.90695357322693  count: 54000\n",
      "time: 55.60600161552429  count: 55000\n",
      "time: 55.264782667160034  count: 56000\n",
      "time: 55.213188886642456  count: 57000\n",
      "time: 55.722151041030884  count: 58000\n",
      "time: 56.48699951171875  count: 59000\n",
      "time: 56.19869112968445  count: 60000\n",
      "time: 55.896180629730225  count: 61000\n",
      "time: 56.39667248725891  count: 62000\n",
      "time: 56.86185050010681  count: 63000\n",
      "time: 56.70806932449341  count: 64000\n",
      "time: 56.60115957260132  count: 65000\n",
      "time: 56.249512910842896  count: 66000\n",
      "time: 56.44808483123779  count: 67000\n",
      "time: 56.24505019187927  count: 68000\n",
      "time: 56.34527826309204  count: 69000\n",
      "time: 55.60507917404175  count: 70000\n",
      "time: 55.806681394577026  count: 71000\n",
      "time: 55.40789771080017  count: 72000\n",
      "time: 56.05290198326111  count: 73000\n",
      "time: 55.45180702209473  count: 74000\n",
      "time: 55.06619668006897  count: 75000\n",
      "time: 55.115925788879395  count: 76000\n",
      "time: 55.21008515357971  count: 77000\n",
      "time: 55.23167157173157  count: 78000\n",
      "Creating .rec file from /home/ec2-user/SageMaker/chestxrayval.lst in /home/ec2-user/SageMaker\n",
      "multiprocessing not available, fall back to single threaded encoding\n",
      "time: 0.05846667289733887  count: 0\n",
      "time: 53.708171367645264  count: 1000\n",
      "time: 53.74234080314636  count: 2000\n",
      "time: 53.39783334732056  count: 3000\n",
      "time: 53.195966720581055  count: 4000\n",
      "time: 53.39307522773743  count: 5000\n",
      "time: 54.070173501968384  count: 6000\n",
      "time: 54.00713562965393  count: 7000\n",
      "time: 54.05080962181091  count: 8000\n",
      "time: 54.591660261154175  count: 9000\n",
      "time: 54.79148626327515  count: 10000\n"
     ]
    }
   ],
   "source": [
    "!python im2rec.py --pack-label chestxraytrain.lst .\n",
    "!python im2rec.py --pack-label chestxrayval.lst ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://analytics-serverless-west/sagemaker/x-ray/train/recordio-pb-data\n",
      "uploaded validation data location: s3://analytics-serverless-west/sagemaker/x-ray/validation/recordio-pb-data\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "bucket = 'analytics-serverless-west'\n",
    "prefix = 'sagemaker/x-ray'\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', 'chestxraytrain.rec')).upload_file('chestxraytrain.rec')\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', 'chestxrayval.rec')).upload_file('chestxrayval.rec')\n",
    "s3_validation_data = 's3://{}/{}/validation/{}'.format(bucket, prefix, key)\n",
    "print('uploaded validation data location: {}'.format(s3_validation_data))\n",
    "\n",
    "# boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', 'chestxraytrain.lst')).upload_file('chestxraytrain.lst')\n",
    "# s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "# print('uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "# boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', 'chestxrayval.lst')).upload_file('chestxrayval.lst')\n",
    "# s3_validation_data = 's3://{}/{}/validation/{}'.format(bucket, prefix, key)\n",
    "# print('uploaded validation data location: {}'.format(s3_validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification results on the chest x-ray dataset\n",
    "\n",
    "We used the ResNet-50 model and first trained the network with 224×224 input image size. We used data augmentation techniques such as random cropping, and image transformations. Even though a chest x-ray image is different from ImageNet images, using a pre-trained model trained on the ImageNet dataset helps in achieving better classification accuracy. Hence, we used the use_pretrained_model hyperparameter in the Amazon SageMaker image classification algorithm to train the network. Since this is a multi-label classification, we set the multi_label parameter to 1. We resized the chest x-ray images to 256 before training so that the network can crop 224×224 regions from the input image.\n",
    "\n",
    "The following code snippet shows how it can be done using the [Amazon SageMaker Estimator interface](https://sagemaker.readthedocs.io/en/latest/estimators.html) and the image classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://analytics-serverless-west/sagemaker/x-ray/train/\n"
     ]
    }
   ],
   "source": [
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "print(s3train)\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "s3validation = 's3://{}/{}/validation/'.format(bucket, prefix)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: image-classification-2018-11-13-19-36-44-842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-13 19:36:44 Starting - Starting the training job...\n",
      "2018-11-13 19:36:47 Starting - Launching requested ML instances............\n",
      "2018-11-13 19:38:50 Starting - Preparing the instances for training.........\n",
      "2018-11-13 19:40:20 Downloading - Downloading input data\n",
      "2018-11-13 19:40:20 Stopping - Stopping the training job\n",
      "2018-11-13 19:40:20 Stopped - Training job stopped\n",
      "..\n",
      "Billable seconds: 1\n"
     ]
    }
   ],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.p3.16xlarge',\n",
    "                        train_volume_size = 50, train_max_run = 360000,\n",
    "                        input_mode= 'File', output_path=s3_output_location,\n",
    "                        sagemaker_session=sess)\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "                                        image_shape = \"3,224,224\", num_classes=14,\n",
    "                                        mini_batch_size=256, \n",
    "                                        resize=256,  epochs=100, \n",
    "                                        learning_rate=0.0005, optimizer='adam', \n",
    "                                        num_training_samples=80000,\n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float32', multi_label = 1)\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with weighted loss\n",
    "\n",
    "An additional feature introduced in image classification is the use of weighted loss to handle class imbalance. Typically, when training with a multi-label dataset, there might be imbalance between classes. This imbalance can lead to a network leaning towards learning one class over another. To avoid that, the Amazon SageMaker image classification algorithm uses the use_weighted_loss hyperparameter to balance the samples. When this parameter is set to 1, a weight value is calculated for each label based on the number of samples of that label in the training set. First, the number of samples in each class is calculated from the training set and the weight for loss update is set to N/N_l for that class where N is the total number of samples in the training set and N_l is the total number of samples for class l in the training set. This will weigh the loss calculated for gradient update differently for each class based on their weight thereby enabling balanced training. The average AUC increased to 0.814 when trained using the weighted loss feature enabled while still using 224×224 input resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.p3.16xlarge',\n",
    "                        train_volume_size = 50, train_max_run = 360000,\n",
    "                        input_mode= 'File', output_path=s3_output_location,\n",
    "                        sagemaker_session=sess)\n",
    "# multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "#                                         image_shape = \"3,224,224\", num_classes=14,\n",
    "#                                         mini_batch_size=256, \n",
    "#                                         resize=256,  epochs=100, \n",
    "#                                         learning_rate=0.0005, optimizer='adam', \n",
    "#                                         num_training_samples=80000,\n",
    "#                                         augmentation_type = 'crop_color_transform',\n",
    "#                                         precision_dtype='float32', multi_label = 1)\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "                                        image_shape = \"3,224,224\", num_classes=14,\n",
    "                                        mini_batch_size=256, resize=256,  epochs=100, \n",
    "                                        learning_rate=0.0005, optimizer='adam', \n",
    "                                        num_training_samples=80000, use_weighted_loss=1,\n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float32', multi_label = 1)\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with mixed-precision\n",
    "\n",
    "The Amazon SageMaker image classification algorithm now supports training in mixed-precision mode. This is controlled by the hyperparameter, precision_dtype, which can be set to ‘float32’ (default) or ‘float16’. In mixed-precision mode, the network computes the backward and forward pass in low-precision (float16) while maintaining the master weights in high-precision (float32). This enables the training to be faster while maintaining similar accuracy. By using the mixed-precision mode, the training time was reduced by 33 percent while obtaining the overall AUC of 0.821, which is similar to the one obtained with float32 training. The training time reduction was even greater when training using two instances for the high-resolution input (see the following section) and increased to 47 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: image-classification-2018-11-04-01-02-01-981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-04 01:02:02 Starting - Starting the training job...\n",
      "2018-11-04 01:02:05 Starting - Launching requested ML instances.........\n",
      "2018-11-04 01:03:37 Starting - Preparing the instances for training......\n",
      "2018-11-04 01:04:45 Downloading - Downloading input data...\n",
      "2018-11-04 01:05:12 Failed - Training job failed\n",
      ".."
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error training image-classification-2018-11-04-01-02-01-981: Failed Reason: ClientError: Data download failed:S3 key: s3://analytics-serverless-west/validation/ matched no files on s3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-75ee87b6761a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                 s3_data_type='S3Prefix')\n\u001b[1;32m     29\u001b[0m \u001b[0mdata_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmultilabel_ic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TrainingJobStatus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Completed'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Stopped'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FailureReason'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(No reason provided)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error training {}: {} Reason: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error training image-classification-2018-11-04-01-02-01-981: Failed Reason: ClientError: Data download failed:S3 key: s3://analytics-serverless-west/validation/ matched no files on s3"
     ]
    }
   ],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.p3.16xlarge',\n",
    "                        train_volume_size = 50, train_max_run = 360000,\n",
    "                        input_mode= 'File', output_path=s3_output_location,\n",
    "                        sagemaker_session=sess)\n",
    "# multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "#                                         image_shape = \"3,224,224\", num_classes=14,\n",
    "#                                         mini_batch_size=256, \n",
    "#                                         resize=256,  epochs=100, \n",
    "#                                         learning_rate=0.0005, optimizer='adam', \n",
    "#                                         num_training_samples=80000,\n",
    "#                                         augmentation_type = 'crop_color_transform',\n",
    "#                                         precision_dtype='float32', multi_label = 1)\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "                                        image_shape = \"3,224,224\", num_classes=14,\n",
    "                                        mini_batch_size=256, resize=256, epochs=100, \n",
    "                                        learning_rate=0.0005, optimizer='adam', \n",
    "                                        num_training_samples=80000, use_weighted_loss=1, \n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float16', multi_label = 1)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with high-resolution input\n",
    "\n",
    "We then used the original input resolution by setting the image_shape parameter to 896×896. We used the use_weighted_loss feature and float32 precision for this training. We used this resolution because it allows the network to sample a 896×896 region from the 1024×1024 during data augmentation. Since the high resolution will use more memory, typically batch_size is reduced to train the network. However, because Amazon SageMaker image classification supports distributed training, we were able to maintain the batch_size by running the training across multiple instances. This is done by setting the [instance_count parameter](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html) in the Amazon SageMaker training to 2. The average AUC for this resolution increased to 0.830, particularly for classes such as nodule, which can benefit from high-resolution input. When we trained with mixed_precision set to 1, the average AUC was 0.825. The training was done using the same code as before but setting the train_instance_count = 2, image_shape=”3,896,896” and not setting the resize parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role, train_instance_count=2,\n",
    "                                                train_instance_type='ml.p3.16xlarge',\n",
    "                                                train_volume_size = 50, train_max_run = 360000,\n",
    "                                                input_mode= 'File', output_path=s3_output_location,\n",
    "                                                sagemaker_session=sess)\n",
    "\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1, \n",
    "                                        image_shape = \"3,896,896\", num_classes=14,\n",
    "                                        mini_batch_size=64, epochs=100, \n",
    "                                        learning_rate=0.00025, optimizer='adam', \n",
    "                                        num_training_samples=80000, use_weighted_loss=1, \n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float32', multi_label = 1)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | 224×224 | 224×224 with class balancing | 224×224 with mixed precision | 896×896 |\n",
    "|-----------| :--------------------------: | :--------------------------: | :-------: | ---------: |\n",
    "|Atelectasis | 0.772 |0.802 | 0.799 | 0.800 |\n",
    "|Cardiomegaly | 0.859 | 0.899 | 0.906 | 0.884 |\n",
    "|Effusion | 0.830 | 0.873 | 0.873 | 0.873 |\n",
    "|Infiltration | 0.626 | 0.693 | 0.691 | 0.698 |\n",
    "|Mass | 0.791 | 0.839 | 0.834 | 0.821 |\n",
    "|Nodule | 0.716 | 0.743 | 0.751 | 0.817 |\n",
    "|Pneumonia | 0.645 | 0.710 | 0.713 | 0.739 |\n",
    "|Pneumothorax | 0.778 | 0.836 | 0.862 | 0.878 |\n",
    "|Consolidation | 0.695 | 0.791 | 0.789 | 0.785 |\n",
    "|Edema | 0.799 | 0.849 | 0.863 | 0.879| \n",
    "|Emphysema | 0.850 | 0.889 | 0.909 | 0.933 |\n",
    "|Fibrosis | 0.764 | 0.791 | 0.811 | 0.822 |\n",
    "|Pleural Thickening | 0.726 | 0.758 | 0.761 | 0.785 |\n",
    "|Hernia | 0.903 | 0.929 | 0.940 | 0.911 |\n",
    "|Average AUC | 0.768 | 0.814 | 0.821 | 0.830 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document.\n",
    "\n",
    "Image-classification only supports encoded .jpg and .png image formats as inference input for now. The output is the probability values for all classes encoded in JSON format, or in JSON Lines format for batch transform.\n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "1. [Create Model](#CreateModel) - Create model for the training output\n",
    "1. [Batch Transform](#BatchTransform) - Create a transform job to perform batch inference.\n",
    "1. [Host the model for realtime inference](#HostTheModel) - Create an inference endpoint and perform realtime inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "We now create a SageMaker Model from the training output. Using the model we can create a Batch Transform Job or an Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-ray-image-classification-model\n",
      "s3://analytics-serverless-west/sagemaker/x-ray/output/image-classification-2018-11-03-07-10-36-441/output/model.tar.gz\n",
      "arn:aws:sagemaker:us-west-2:649037252677:model/x-ray-image-classification-model\n",
      "CPU times: user 44.8 ms, sys: 0 ns, total: 44.8 ms\n",
      "Wall time: 407 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "# get the name of the training job completed below for this variable\n",
    "job_name=\"image-classification-2018-11-03-07-10-36-441\"\n",
    "model_name = \"x-ray-image-classification-model\"\n",
    "print(model_name)\n",
    "info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "hosting_image = get_image_uri(boto3.Session().region_name, 'image-classification')\n",
    "\n",
    "primary_container = {\n",
    "    'Image': hosting_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realtime inference\n",
    "\n",
    "We now host the model with an endpoint and perform realtime inference.\n",
    "\n",
    "This section involves several steps,\n",
    "1. [Create endpoint configuration](#CreateEndpointConfiguration) - Create a configuration defining an endpoint.\n",
    "1. [Create endpoint](#CreateEndpoint) - Use the configuration to create an inference endpoint.\n",
    "1. [Perform inference](#PerformInference) - Perform inference on some input data using the endpoint.\n",
    "1. [Clean up](#CleanUp) - Delete the endpoint and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Configuration\n",
    "At launch, we will support configuring REST endpoints in hosting with multiple models, e.g. for A/B testing purposes. In order to support this, customers create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way.\n",
    "\n",
    "In addition, the endpoint configuration describes the instance type required for model deployment, and at launch will describe the autoscaling configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint configuration name: x-ray-image-classification-model-epc--2018-11-03-16-23-12\n",
      "Endpoint configuration arn:  arn:aws:sagemaker:us-west-2:649037252677:endpoint-config/x-ray-image-classification-model-epc--2018-11-03-16-23-12\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = model_name + '-epc-' + timestamp\n",
    "endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint\n",
    "Lastly, the customer creates the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 9-11 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: x-ray-image-classification-model-ep--2018-11-03-16-26-35\n",
      "EndpointArn = arn:aws:sagemaker:us-west-2:649037252677:endpoint/x-ray-image-classification-model-ep--2018-11-03-16-26-35\n",
      "CPU times: user 29.7 ms, sys: 0 ns, total: 29.7 ms\n",
      "Wall time: 373 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "sage = boto3.client('sagemaker')\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = model_name + '-ep-' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sage.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointStatus = Creating\n",
      "Endpoint creation ended with EndpointStatus = InService\n"
     ]
    }
   ],
   "source": [
    "# get the status of the endpoint\n",
    "response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "\n",
    "# wait until the status has changed\n",
    "sage.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message,\n",
    "\n",
    "> `Endpoint creation ended with EndpointStatus = InService`\n",
    "\n",
    "then congratulations! You now have a functioning inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console.\n",
    "\n",
    "We will finally create a runtime object from which we can invoke the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Inference\n",
    "Finally, the customer can now validate the model for use. They can obtain the endpoint from the client library using the result from previous operations, and generate classifications from the trained model using that endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000005_000.png\n",
      "1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000005_001.png\n",
      "2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000005_002.png\n",
      "3\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000005_003.png\n",
      "4\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000005_004.png\n",
      "5\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000005_005.png\n",
      "6\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000005_006.png\n",
      "7\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\timages/00000005_007.png\n",
      "8\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000007_000.png\n",
      "9\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\timages/00000010_000.png\n"
     ]
    }
   ],
   "source": [
    "!head -10 chestxraytest.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = ! aws s3 ls s3://analytics-serverless-west/sagemaker/x-ray/images/ | cut -d \" \" -f 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect Matches:  12\n",
      "Perfect Match, no labels:  5442\n",
      "All labels correct with false positives:  0\n",
      "Some labels correct with no false positives:  7\n",
      "No labels correct with no false positives:  3957\n",
      "Some labels correct with false positives:  0\n",
      "No labels correct with false positives:  582\n",
      "score:  1003.0\n",
      "total processed:  10000\n",
      "weighted score: 0.6445\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "detection=[]\n",
    "detections=[]\n",
    "totalprocessed=0\n",
    "perfect=0\n",
    "perfectnl=0\n",
    "acwfp=0\n",
    "scwnfp=0\n",
    "ncwnfp=0\n",
    "scwfp=0\n",
    "ncwfp=0\n",
    "\n",
    "with open('Data_Entry_2017.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    csv_reader.__next__()\n",
    "    for row in csv_reader:\n",
    "        ! aws s3 cp s3://analytics-serverless-west/sagemaker/x-ray/images/{row[0]} . --quiet\n",
    "        with open(row[0], 'rb') as f:\n",
    "            payload = f.read()\n",
    "            payload = bytearray(payload)\n",
    "        endpoint_name = 'x-ray-image-classification-model-ep--2018-11-03-16-26-35'\n",
    "        response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                           ContentType='application/x-image', \n",
    "                                           Body=payload)\n",
    "        result = response['Body'].read()\n",
    "        # result will be in json format and convert it to ndarray\n",
    "        result = json.loads(result)\n",
    "        # the result will output the probabilities for all classes\n",
    "        # find the class with maximum probability and print the class index\n",
    "        index = np.argmax(result)\n",
    "        disease_list = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', \\\n",
    "                           'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', \\\n",
    "                           'Hernia']\n",
    "\n",
    "        detection=[]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Original thresholds\n",
    "        for idx, val in enumerate(result):\n",
    "           switcher1 = {\n",
    "               \"Atelectasis\": 0.17162750661373138,\n",
    "               \"Consolidation\": 0.13634157180786133,\n",
    "               \"Infiltration\": 0.17990991473197937,\n",
    "               \"Pneumothorax\": 0.08858224004507065,\n",
    "               \"Edema\": 0.066930390894413,\n",
    "               \"Emphysema\": 0.03022913448512554,\n",
    "               \"Fibrosis\": 0.02981789968907833,\n",
    "               \"Effusion\": 0.24083951115608215,\n",
    "               \"Pneumonia\": 0.012233437038958073,\n",
    "               \"Pleural_Thickening\": 0.08263493329286575,\n",
    "               \"Cardiomegaly\": 0.13562433421611786,\n",
    "               \"Nodule\": 0.047457192093133926,\n",
    "               \"Mass\": 0.034243155270814896,\n",
    "               \"Hernia\": 0.011998575180768967,\n",
    "           }\n",
    "\n",
    "        \n",
    "#        Original thresholds * 4\n",
    "#        for idx, val in enumerate(result):\n",
    "#            switcher1 = {\n",
    "#                \"Atelectasis\": 0.686510026454924,\n",
    "#                \"Consolidation\": 0.545366287231444,\n",
    "#                \"Infiltration\": 0.719639658927916,\n",
    "#                \"Pneumothorax\": 0.354328960180282,\n",
    "#                \"Edema\": 0.267721563577652,\n",
    "#                \"Emphysema\": 0.120916537940502,\n",
    "#                \"Fibrosis\": 0.119271598756313,\n",
    "#                \"Effusion\": 0.963358044624328,\n",
    "#                \"Pneumonia\": 0.048933748155832,\n",
    "#                \"Pleural_Thickening\": 0.330539733171463,\n",
    "#                \"Cardiomegaly\": 0.542497336864468,\n",
    "#                \"Nodule\": 0.189828768372536,\n",
    "#                \"Mass\": 0.136972621083259,\n",
    "#                \"Hernia\": 0.047994300723076,\n",
    "#            }\n",
    "\n",
    "\n",
    "        if (result[idx] >= (switcher1.get(disease_list[idx], 1))):\n",
    "#            print(disease_list[idx]) \n",
    "#            print(switcher.get(disease_list[idx], 1)*2)\n",
    "#            print(switcher.get(disease_list[idx], 1))\n",
    "            detection.append(disease_list[idx])\n",
    "        correctlabelcount=0\n",
    "        for label in row[1].split(\"|\"):\n",
    "            if label in detection:\n",
    "                correctlabelcount=correctlabelcount+1\n",
    "        printed=0\n",
    "        if (correctlabelcount == len(row[1].split(\"|\")) and len(row[1].split(\"|\")) == len(detection)):\n",
    "            results.append(\"Perfect Match\")\n",
    " #           print(row[0], \" Perfect Match\")\n",
    " #           print(\"Detected: \", detection)\n",
    " #           print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            perfect=perfect+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if (len(detection) == 0 and row[1] == \"No Finding\"):\n",
    "            results.append(\"Perfect Match, No Findings\")\n",
    " #           print(row[0], \" Perfect Match, No Findings\")\n",
    " #           print(\"Detected: \", detection)\n",
    " #           print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            perfectnl=perfectnl+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if (correctlabelcount == len(row[1].split(\"|\")) and len(row[1].split(\"|\")) < len(detection)):\n",
    "            results.append(\"Detected all correct labels, with false positives\")\n",
    " #           print(row[0], \" Detected all correct labels, with false positives\")\n",
    " #           print(\"Detected: \", detection)\n",
    " #           print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            acwfp=acwfp+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if (correctlabelcount < len(row[1].split(\"|\")) and correctlabelcount != 0 and correctlabelcount == len(detection)):\n",
    "            results.append(\"Detected some correct labels, with no false positives\")\n",
    " #           print(row[0], \" Detected some correct labels, with no false positives\")\n",
    " #           print(\"Detected: \", detection)\n",
    " #           print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            scwnfp=scwnfp+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if (len(detection) == 0 and row[1] != \"No Finding\"):\n",
    "            results.append(\"Detected no correct labels, with no false positives\")\n",
    "#            print(row[0], \" Detected no correct labels, with no false positives\")\n",
    "#            print(\"Detected: \", detection)\n",
    "#            print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            ncwnfp=ncwnfp+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if (correctlabelcount < len(row[1].split(\"|\")) and correctlabelcount != 0 and correctlabelcount < len(detection)):\n",
    "            results.append(\"Detected some correct labels, with false positives\")\n",
    "#            print(row[0], \" Detected some correct labels, with false positives\")\n",
    "#            print(\"Detected: \", detection)\n",
    "#            print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            scwfp=scwfp+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if (correctlabelcount == 0 and len(detection) != 0 and row[1] != \"No Finding\"):\n",
    "            results.append(\"Detected no correct labels, with false positives\")\n",
    "#            print(row[0], \" Detected no correct labels, with false positives\")\n",
    "#            print(\"Detected: \", detection)\n",
    "#            print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            ncwfp=ncwfp+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if (len(detection) > 0 and row[1] == \"No Finding\"):\n",
    "            results.append(\"Detected no correct labels, with false positives\")\n",
    "#            print(row[0], \" Detected no correct labels, with false positives\")\n",
    "#            print(\"Detected: \", detection)\n",
    "#            print(\"Actual:\", row[1].split(\"|\"))\n",
    "            printed=1\n",
    "            ncwfp=ncwfp+1\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if printed == 0:\n",
    "#            print(row[0], \" Didn't match any scenarios\")\n",
    "#            print(\"Detected: \", detection)\n",
    "#            print(\"Actual:\", row[1].split(\"|\"))\n",
    "            totalprocessed=totalprocessed+1\n",
    "        if totalprocessed == 10000:\n",
    "            break\n",
    "\n",
    "        ! rm {row[0]}\n",
    "print(\"Perfect Matches: \", perfect)\n",
    "print(\"Perfect Match, no labels: \", perfectnl)\n",
    "print(\"All labels correct with false positives: \", acwfp)\n",
    "print(\"Some labels correct with no false positives: \", scwnfp)\n",
    "print(\"No labels correct with no false positives: \", ncwnfp)\n",
    "print(\"Some labels correct with false positives: \", scwfp)\n",
    "print(\"No labels correct with false positives: \", ncwfp)\n",
    "print(\"score: \", (perfect*1)+(acwfp*0.5)+(scwnfp*0.25)+(ncwnfp*0.25)+(scwfp*0.25))\n",
    "print(\"total processed: \", totalprocessed)\n",
    "print(\"weighted score:\", ((perfect*1)+(perfectnl*1)+(acwfp*0.5)+(scwnfp*0.25)+(ncwnfp*0.25)+(scwfp*0.25))/totalprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cardiomegaly', 'Emphysema']\n",
      "00000001_002.png\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Data_Entry_2017.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    csv_reader.__next__()\n",
    "    csv_reader.__next__()\n",
    "    print(csv_reader.__next__()[1].split(\"|\"))\n",
    "    print(csv_reader.__next__()[0])\n",
    "#    for row in csv_reader:\n",
    "#            print(f'\\t{row[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  10.75\n",
      "total processed:  50\n",
      "weighted score: 0.215\n"
     ]
    }
   ],
   "source": [
    "print(\"score: \", score)\n",
    "print(\"total processed: \", totalprocessed)\n",
    "print(\"weighted score:\", score/totalprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Original thresholds\n",
    "        for idx, val in enumerate(result):\n",
    "           switcher1 = {\n",
    "               \"Atelectasis\": 0.17162750661373138,\n",
    "               \"Consolidation\": 0.13634157180786133,\n",
    "               \"Infiltration\": 0.17990991473197937,\n",
    "               \"Pneumothorax\": 0.08858224004507065,\n",
    "               \"Edema\": 0.066930390894413,\n",
    "               \"Emphysema\": 0.03022913448512554,\n",
    "               \"Fibrosis\": 0.02981789968907833,\n",
    "               \"Effusion\": 0.24083951115608215,\n",
    "               \"Pneumonia\": 0.012233437038958073,\n",
    "               \"Pleural_Thickening\": 0.08263493329286575,\n",
    "               \"Cardiomegaly\": 0.13562433421611786,\n",
    "               \"Nodule\": 0.047457192093133926,\n",
    "               \"Mass\": 0.034243155270814896,\n",
    "               \"Hernia\": 0.011998575180768967,\n",
    "           }\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        # 20% false positive rate in each class thresholds\n",
    "        for idx, val in enumerate(result):\n",
    "           switcher1 = {\n",
    "               \"Atelectasis\": 0.778095006942749,\n",
    "               \"Consolidation\": 1.2920989692211151,\n",
    "               \"Infiltration\": 1.4605742394924164,\n",
    "               \"Pneumothorax\": 0.6257116198539734,\n",
    "               \"Edema\": 0.2713622748851776,\n",
    "               \"Emphysema\": 0.7842891812324524,\n",
    "               \"Fibrosis\": 0.39154180884361267,\n",
    "               \"Effusion\": 0.6497185230255127,\n",
    "               \"Pneumonia\": 1.0556025952100754,\n",
    "               \"Pleural_Thickening\": 1.3903022706508636,\n",
    "               \"Cardiomegaly\": 0.7538480162620544,\n",
    "               \"Nodule\": 1.3643558323383331,\n",
    "               \"Mass\": 1.2591769099235535,\n",
    "               \"Hernia\": 0.05428028479218483,\n",
    "           }        \n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
